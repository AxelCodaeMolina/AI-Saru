{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**AI Saru: a tool for macaques detection and identification**\n",
        "\n",
        "This is the tool developed in the 2024 Primates article *Deep Learning for Automatic Detection and Facial Recognition in Japanese Macaques: Illuminating Social Networks* by Paulet et al., 2024. You can find it here:  https://doi.org/10.1007/s10329-024-01137-5 and the preprint is here: https://doi.org/10.48550/arXiv.2310.06489 .\n",
        "\n",
        "Please note that this is still a work in project, we will do our best to provide a simple beginner friendly code so that anyone can use it and expend on it. If you need any help running the code or if you have any question feel free to contact Axel Molina by email using either molina.axel@ens.psl.eu or molina.axel.pro@gmail.com . You should not need any specialist knowledge to run this code."
      ],
      "metadata": {
        "id": "bU8YBOGyUSWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**What this will do**\n",
        "\n",
        "This tool will take your **Japanese macaques** videos, **detect the faces** of any japanese macaques on it, **track the faces** across the video and **identify the individuals** faces.\n",
        "\n",
        "For the moment this tool can identify the 2023 **Koshima** population (famous for its potatoe washing !) and the 2024 **Shodoshima** population (famous for its \"sarudango\" !) .\n",
        "\n",
        "It will also give you simple **co-occurrence matrices** showing how many times two individuals appear together on video. There is different options for the matrices. Details are provided at the end of this colab.\n",
        "\n",
        "They can then be used easily with Gephi to create visual representations of social networks, or with R for statistics.\n",
        "\n",
        "This code also generate videos with annotation, if you want to see the model track your macaques faces. In futur versions this will be optional."
      ],
      "metadata": {
        "id": "r3UBllmTaiaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What you will need**\n",
        "\n",
        "#A Google Drive\n",
        "\n",
        "To use this tool you will need to have, in a Google Drive:\n",
        "- the weights of a **trained model for macaques detection** in .pt format\n",
        "- the weights of a **trained model for macaques identification** in .pt format\n",
        "- **videos** of your Japanese macaques in .mp4 format\n",
        "\n",
        "You can find the .pt files for our AIs on the **GitHub** of the project here, along with some exemple videos: https://github.com/AxelCodaeMolina/AI-Saru . You can also use your own.\n",
        "\n",
        "This code assume that your Drive is organized as follows:\n",
        "- a folder in your drive named \"**AISaru**\"\n",
        "- a subfolder in the \"AISaru\" folder named \"**models**\"\n",
        "- another subfolder in the \"AISaru\" folder named \"**videos**\"\n",
        "- (the code should also create a folder named \"output\")\n",
        "\n",
        "#A few clicks\n",
        "\n",
        "Once you have that, the code will be easy to run: simply press the arrows on the left of every block of code. I recommand you wait until you see a little greeen arrow on the side of the block of code to click on the next arrow. If anything crashes, simply refresh the page and re-run everything.\n",
        "\n",
        "The code may take a few hours to run, depending on the number of videos.\n",
        "\n",
        "With a few adjustments this can also run on a computer using Python."
      ],
      "metadata": {
        "id": "rwoOSyJLV83z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 0: connect the Drive and load the packages needed**"
      ],
      "metadata": {
        "id": "dADQwfkcdeVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first let's connect your well organized drive to this code, just click on the left grey arrow !\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')\n"
      ],
      "metadata": {
        "id": "bsEDZarzdOj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now just click on the arrow here, this will load all the packages we need\n",
        "\n",
        "# Install supervision for the tracker\n",
        "!pip install supervision\n",
        "import supervision as sv\n",
        "\n",
        "# Install ultranalytics to get the models\n",
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "# Import numpy\n",
        "import numpy as np\n",
        "# Import panda\n",
        "import pandas as pd\n",
        "# Import os\n",
        "import os\n",
        "# Import intertools combinations\n",
        "from itertools import combinations\n",
        "# Import files\n",
        "from google.colab import files\n",
        "\n",
        "# Install FFmpeg\n",
        "!apt-get update -qq && apt-get install -y ffmpeg"
      ],
      "metadata": {
        "id": "QmwzS8vXe_dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect ! Now let's dive into it."
      ],
      "metadata": {
        "id": "WatbTYi4ddeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 1: processing the videos**"
      ],
      "metadata": {
        "id": "IOxBi1-RaBku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the longest and hardest part. First we will define some variables and functions for the rest of the code. Then we will use them on the videos in the drive.\n",
        "\n",
        "If you are using a custom model this is where you should edit the code: simply remplace the \"/drive/MyDrive/AISaru/models/ID.pt\" in the first block of code with the path to the .pt file you will use.  "
      ],
      "metadata": {
        "id": "9uLg4MOhc8_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we define the variables and functions needed for\n",
        "# both the tracking and creation of a matrix with the results\n",
        "\n",
        "# First we specify where are the models we want to use for detection and classification\n",
        "detector = YOLO(\"/drive/MyDrive/AISaru/models/DET.pt\")\n",
        "classifier = YOLO(\"/drive/MyDrive/AISaru/models/ID.pt\")\n",
        "\n",
        "# Then we define the tracker and annotator from the supervison package\n",
        "byte_tracker = sv.ByteTrack()\n",
        "annotator = sv.BoxAnnotator()\n",
        "\n",
        "# And some variables for the following functions\n",
        "data = []\n",
        "detection_index = 0\n",
        "video = \"\""
      ],
      "metadata": {
        "id": "ybwIyw7ZZuL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the \"aisaru\" callback function that will be used to detect, track\n",
        "# and identify the macaques, and to create a matrix\n",
        "\n",
        "def aisaru(frame: np.ndarray, index: int) -> np.ndarray:\n",
        "\n",
        "    global video, detection_index  # Specify that you are using the global variables\n",
        "    results = detector(frame)[0]\n",
        "    detections = sv.Detections.from_ultralytics(results)\n",
        "    detections = byte_tracker.update_with_detections(detections)\n",
        "    labels = [\n",
        "    f\"#{tracker_id} {detections.data['class_name'][class_id]} {confidence:0.2f}\"\n",
        "    for confidence, class_id, tracker_id\n",
        "    in zip(detections.confidence, detections.class_id, detections.tracker_id)\n",
        "    ]\n",
        "\n",
        "    if detections: # This is because if there is no detection, we are not interested in running a prediction\n",
        "        bbox = detections.xyxy\n",
        "        for bbox_item, label in zip(bbox, labels):\n",
        "            x1, y1, x2, y2 = map(int, bbox_item) # First we define the region of interest for classification\n",
        "            tracker_id = int(label.split()[0][1:])\n",
        "            class_name = label.split()[1]\n",
        "            confidence = float(label.split()[2])\n",
        "            roi = frame[y1:y2, x1:x2]\n",
        "            results_classification = classifier(roi)[0]  # Predict on an image\n",
        "            classifications = sv.Classifications.from_ultralytics(results_classification)\n",
        "            classifications.get_top_k(5) # This will take the 5 best prediction for the image\n",
        "            top_k_class_ids, top_k_confidences = classifications.get_top_k(5)\n",
        "            class_names = results_classification.names\n",
        "            top_k_class_names = [class_names[class_id] for class_id in top_k_class_ids]\n",
        "            for class_id, class_name, confidence in zip(top_k_class_ids, top_k_class_names, top_k_confidences): # Here we create the labels for the DataFrame\n",
        "                data.append({\n",
        "                    \"DetectionID\": detection_index,\n",
        "                    \"Video\": video,\n",
        "                    \"Track\": tracker_id,\n",
        "                    \"Frame\": index,\n",
        "                    \"Confidence\": confidence,\n",
        "                    \"Box\": bbox_item,\n",
        "                    \"NameID\": class_name,\n",
        "                    \"ConfidenceID\": confidence,\n",
        "                    \"ClassID\": class_id,\n",
        "                })\n",
        "            detection_index += 1 # Increment the detection index for the next detection\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "    return annotator.annotate(scene=frame.copy(), detections=detections, labels=labels)"
      ],
      "metadata": {
        "id": "NyHDXXBVfhD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the function using the aisaru callback\n",
        "# and processing each video file of the video Drive folder\n",
        "\n",
        "def aisaru_allvideos(directory):\n",
        "    global video\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".mp4\"):\n",
        "            print(filename)\n",
        "            source_path = os.path.join(directory, filename)\n",
        "            target_filename = os.path.splitext(filename)[0] + \"_output.mp4\" # This is the name of the new version of the video, with the detections\n",
        "            target_path = directory + \"/output/\" + target_filename\n",
        "            video = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Process the videos\n",
        "            sv.process_video(source_path=source_path, target_path=target_path, callback=aisaru)\n",
        "\n",
        "            # Generate matrix and save it to Excel\n",
        "            df = pd.DataFrame(data)\n",
        "            excel_filename = os.path.splitext(filename)[0] + \".xlsx\"\n",
        "            df.to_excel(os.path.join(directory + \"/output/\", excel_filename), index=False)\n",
        "\n",
        "# This should be the path to your directory containing the videos\n",
        "directory_path = \"/drive/MyDrive/AISaru/videos\""
      ],
      "metadata": {
        "id": "tQ6vMv7BgIvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now it is time to try our tool on the videos."
      ],
      "metadata": {
        "id": "oY_nnAICi_eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the code for the actual video processing, this will probably take time\n",
        "aisaru_allvideos(directory_path)"
      ],
      "metadata": {
        "id": "S5PZKMLfhbvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 2: creating the co-occurrence matrices**"
      ],
      "metadata": {
        "id": "YdSMnpuIaSLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our results of the video processing, we will clean and edit the output excel file to have a good co-occurrence matrix."
      ],
      "metadata": {
        "id": "N7IFgqSpjM2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But first we need an empty co-occurences matrix to fill."
      ],
      "metadata": {
        "id": "Feia3QqqkY65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will create a co-occurrence matrix for the individuals of your classifier\n",
        "\n",
        "results_classification = classifier(\"/drive/MyDrive/AISaru/Yotsuba.jpg\")[0] # Perform an identifiation on a random image (change the path to one of yours) of your drive to then execute the rest of the code\n",
        "class_names = results_classification.names\n",
        "\n",
        "# Extract class names from the dictionary values\n",
        "class_names = list(class_names.values())\n",
        "\n",
        "# Create a DataFrame with class names as both rows and columns\n",
        "co_occurrence_matrix = pd.DataFrame(0, index=class_names, columns=class_names)\n",
        "\n",
        "# Save it\n",
        "co_occurrence_matrix.to_excel(\"/drive/MyDrive/AISaru/output/co_occurrence_matrix.xlsx\")"
      ],
      "metadata": {
        "id": "Bj2QwIYXkX3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect, now let's edit the Excel files of the results."
      ],
      "metadata": {
        "id": "wiCqKg9yksFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the function that compile all the Excel files from the videos in the directory\n",
        "# into one (potentially very) long Excel file\n",
        "\n",
        "def compile_excel_files(directory):\n",
        "    # Initialize an empty DataFrame to store all data\n",
        "    all_data = pd.DataFrame()\n",
        "\n",
        "    # Iterate over each file in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".xlsx\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "\n",
        "            # Read the Excel file into a DataFrame\n",
        "            df = pd.read_excel(file_path)\n",
        "\n",
        "            # Append the contents of the DataFrame to all_data\n",
        "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
        "            all_data = all_data.drop_duplicates()\n",
        "            print(file_path)\n",
        "\n",
        "    # Save all_data to a single Excel file\n",
        "    compiled_file_path = os.path.join(directory.replace(\"/output\", \"\"), \"compiled_output.xlsx\")\n",
        "    all_data.to_excel(compiled_file_path, index=False)\n",
        "\n",
        "# This is the path to your directory containing all the Excel files\n",
        "directory_path = \"/drive/MyDrive/AISaru/output\"\n",
        "\n",
        "# This is the execution of the function\n",
        "compile_excel_files(directory_path)"
      ],
      "metadata": {
        "id": "FreZx1nwjX52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will give us the all potential IDs for each tracks\n",
        "\n",
        "df = pd.read_excel(\"/drive/MyDrive/AISaru/compiled_output.xlsx\")\n",
        "\n",
        "# Group by both 'Track' and 'NameID' and calculate the sum of 'ConfidenceID' for each group\n",
        "sum_confidence_per_name = df.groupby(['Track', 'NameID', 'Video'])['ConfidenceID'].sum().reset_index()\n",
        "\n",
        "# Get the index of the row with the highest 'ConfidenceID' for each 'Track'\n",
        "max_confidence_idx = sum_confidence_per_name.groupby('Track')['ConfidenceID'].idxmax()\n",
        "\n",
        "# Filter the DataFrame to keep only the rows with the highest 'ConfidenceID' for each 'Track'\n",
        "df_best_id_per_track = sum_confidence_per_name.loc[max_confidence_idx]\n",
        "\n",
        "# Group by 'Track' and find the minimum and maximum values of 'Frame' for each group\n",
        "first_last_frame_per_track = df.groupby(['Track', 'Video'])['Frame'].agg(['min', 'max']).reset_index()\n",
        "\n",
        "# Merge the two DataFrames on the 'Track' and 'Video' columns\n",
        "combined_df = pd.merge(df_best_id_per_track, first_last_frame_per_track, on=['Track', 'Video'])\n",
        "\n",
        "# Save the resulting DataFrame\n",
        "combined_df.to_excel(\"/drive/MyDrive/AISaru/TracksID.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "HUZyGnZ8kEcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we clean and simplify the compiled output to be able to use it in the next steps\n",
        "# (This file is actually small and clear, you can open it !)\n",
        "\n",
        "df = pd.read_excel(\"/drive/MyDrive/AISaru/compiled_output.xlsx\")\n",
        "\n",
        "cleaned_output = df.drop(['Box', 'Confidence', 'ClassID'], axis=1)\n",
        "cleaned_output = cleaned_output.drop_duplicates(subset=['DetectionID', 'Video'])\n",
        "\n",
        "# Create dictionaries mapping 'Track' to 'NameID' and 'ConfidenceID' in combined_df\n",
        "track_to_nameid = combined_df.set_index('Track')['NameID'].to_dict()\n",
        "track_to_confidenceid = combined_df.set_index('Track')['ConfidenceID'].to_dict()\n",
        "\n",
        "# Replace 'NameID' and 'ConfidenceID' in cleaned_output using the mappings\n",
        "cleaned_output['NameID'] = cleaned_output['Track'].map(track_to_nameid)\n",
        "cleaned_output['ConfidenceID'] = cleaned_output['Track'].map(track_to_confidenceid)\n",
        "\n",
        "# Save the cleaned DataFrame\n",
        "cleaned_output.to_excel(\"/drive/MyDrive/AISaru/output/cleaned_output.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "ouYAPU5ikylL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's fill and edit the matrix based on the compiled output file ! There is multiple methods that you can use.\n",
        "\n",
        "You can have the co-occurrence on a frame (if your video is filmed in such a way that some macaques are visible but not in social contact) or by video (if the videos are close up centered around one individual, like we did)."
      ],
      "metadata": {
        "id": "z2a-ez9hlu0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code to do it by videos:"
      ],
      "metadata": {
        "id": "UyjNM5GnmoSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will create an updated co-occurence matrix\n",
        "# using the compiled output file, using co-occurences by videos\n",
        "\n",
        "# Load the cleaned DataFrame\n",
        "cleaned_output = pd.read_excel(\"/drive/MyDrive/AISaru/output/cleaned_output.xlsx\")\n",
        "\n",
        "# Load the co-occurrence matrix DataFrame\n",
        "co_occurrence_matrix = pd.read_excel(\"/drive/MyDrive/AISaru/output/co_occurrence_matrix.xlsx\", index_col=0)\n",
        "\n",
        "# Iterate through each unique video\n",
        "for video, group in cleaned_output.groupby('Video'):\n",
        "    # Check if there are multiple NameIDs for the same video\n",
        "    if len(group) > 1:\n",
        "        # Extract the unique NameIDs for this video\n",
        "        name_ids = group['NameID'].unique()\n",
        "\n",
        "        # Update the corresponding cells in the co-occurrence matrix for each pair of NameIDs\n",
        "        for name_id1, name_id2 in combinations(name_ids, 2):\n",
        "            co_occurrence_matrix.at[name_id1, name_id2] += 1\n",
        "            co_occurrence_matrix.at[name_id2, name_id1] += 1\n",
        "\n",
        "# Save the updated co-occurrence matrix to a new Excel file\n",
        "co_occurrence_matrix.to_excel(\"/drive/MyDrive/AISaru/output/co_occurrence_matrix_updatedVID.xlsx\")\n",
        "files.download(\"/drive/MyDrive/AISaru/output/co_occurrence_matrix_updatedVID.xlsx\")"
      ],
      "metadata": {
        "id": "pJiU-ymTmvxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will normalize the co-occurence matrix\n",
        "\n",
        "# List the number of unique videos for each NameID\n",
        "cleaned_output_df = pd.read_excel(\"/drive/MyDrive/AISaru/output/cleaned_output.xlsx\")\n",
        "videos_per_nameid = cleaned_output_df.groupby('NameID')['Video'].nunique().to_dict()\n",
        "\n",
        "# Extract the number of co-occurrences for each unique pair of NameID\n",
        "co_occurrence_matrix_df = pd.read_excel(\"/drive/MyDrive/AISaru/output/matrix_VID.xlsx\", index_col=0)\n",
        "\n",
        "# Calculate the ratio\n",
        "for name_id1 in co_occurrence_matrix_df.index:\n",
        "    for name_id2 in co_occurrence_matrix_df.columns:\n",
        "        if name_id1 != name_id2:\n",
        "            num_co_occurrences = co_occurrence_matrix_df.at[name_id1, name_id2]\n",
        "            num_videos_nameid1 = videos_per_nameid.get(name_id1, 0)\n",
        "            num_videos_nameid2 = videos_per_nameid.get(name_id2, 0)\n",
        "            total_videos = num_videos_nameid1 + num_videos_nameid2\n",
        "            ratio = num_co_occurrences / total_videos\n",
        "            co_occurrence_matrix_df.at[name_id1, name_id2] = ratio\n",
        "\n",
        "# Update the co-occurrence matrix\n",
        "co_occurrence_matrix_df.to_excel(\"/drive/MyDrive/AISaru/output/matrix_normalized_VID.xlsx\")\n",
        "\n",
        "# Save the updated co-occurrence matrix\n",
        "files.download(\"/drive/MyDrive/AISaru/output/matrix_normalized_VID.xlsx\")\n"
      ],
      "metadata": {
        "id": "Vsm2a3mQpio5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code to do it by frames:"
      ],
      "metadata": {
        "id": "J6zzL-plmrg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will create an updated co-occurence matrix\n",
        "# using the compiled output file, using co-occurences by frames\n",
        "\n",
        "# Load the cleaned DataFrame\n",
        "cleaned_output = pd.read_excel(\"/drive/MyDrive/AISaru/output/cleaned_output.xlsx\")\n",
        "\n",
        "# Load the co-occurrence matrix DataFrame\n",
        "co_occurrence_matrix = pd.read_excel(\"/drive/MyDrive/AISaru/output/co_occurrence_matrix.xlsx\", index_col=0)\n",
        "\n",
        "# Iterate through each unique combination of Video and Frame number\n",
        "for (video, frame), group in cleaned_output.groupby(['Video', 'Frame']):\n",
        "    # Check if there are multiple NameIDs for the same Video and Frame number\n",
        "    if len(group) > 1:\n",
        "        # Extract the NameIDs for this combination\n",
        "        name_ids = group['NameID'].unique()\n",
        "\n",
        "        # Avoid counting co-occurrences if the NameIDs are the same\n",
        "        if name_id1 != name_id2:\n",
        "          # Update the corresponding cells in the co-occurrence matrix for each pair of NameIDs\n",
        "          for name_id1, name_id2 in combinations(name_ids, 2):\n",
        "              co_occurrence_matrix.at[name_id1, name_id2] += 1\n",
        "              co_occurrence_matrix.at[name_id2, name_id1] += 1\n",
        "\n",
        "# Save the updated co-occurrence matrix\n",
        "co_occurrence_matrix.to_excel(\"/drive/MyDrive/AISaru/output/co_occurrence_matrix_updatedFRAMES.xlsx\")"
      ],
      "metadata": {
        "id": "ut9xQXiUmwL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will normalize the co-occurence matrix\n",
        "\n",
        "# List the number of unique videos for each NameID\n",
        "cleaned_output_df = pd.read_excel(\"/drive/MyDrive/AISaru/output/cleaned_output.xlsx\")\n",
        "videos_per_nameid = cleaned_output_df.groupby('NameID')['Video'].nunique().to_dict()\n",
        "\n",
        "# Extract the number of co-occurrences for each unique pair of NameID\n",
        "co_occurrence_matrix_df = pd.read_excel(\"/drive/MyDrive/AISaru/output/matrix_FRAMES.xlsx\", index_col=0)\n",
        "\n",
        "# Calculate the ratio\n",
        "for name_id1 in co_occurrence_matrix_df.index:\n",
        "    for name_id2 in co_occurrence_matrix_df.columns:\n",
        "        if name_id1 != name_id2:\n",
        "            num_co_occurrences = co_occurrence_matrix_df.at[name_id1, name_id2]\n",
        "            num_videos_nameid1 = videos_per_nameid.get(name_id1, 0)\n",
        "            num_videos_nameid2 = videos_per_nameid.get(name_id2, 0)\n",
        "            total_videos = num_videos_nameid1 + num_videos_nameid2\n",
        "            ratio = num_co_occurrences / total_videos\n",
        "            co_occurrence_matrix_df.at[name_id1, name_id2] = ratio\n",
        "\n",
        "# Update the co-occurrence matrix\n",
        "co_occurrence_matrix_df.to_excel(\"/drive/MyDrive/AISaru/output/matrix_normalized_FRAMES.xlsx\")\n"
      ],
      "metadata": {
        "id": "2bseX0qHpaWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're done ! You should now have :\n",
        "- normalized matrices to use in Gephi or R for social network modeling (matrix_normalizedXXX.xlsx)\n",
        "- a list of detection and identification (cleaned_output.xlsx)"
      ],
      "metadata": {
        "id": "qE_lQgUPYcVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Credits**"
      ],
      "metadata": {
        "id": "cRJN_lQwZOpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to use this code for your work, please cite the article by Paulet et al. mentioned at the start of this colab and provide a link to the github of this project, where this colab can be found: (SOON)\n",
        "\n",
        "Contact me if you need anything ! Good luck !"
      ],
      "metadata": {
        "id": "xN6in50TZhgs"
      }
    }
  ]
}