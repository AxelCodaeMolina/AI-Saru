{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AxelCodaeMolina/AI-Saru/blob/main/Molina_Paulet_AISaru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Face detection and classification of Koshima macaques"
      ],
      "metadata": {
        "id": "7gib2oq99Oyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code used in the 10/10/2023 version of the preprint **Deep Learning for Automatic Detection and Facial Recognition in Japanese Macaques: Illuminating Social Networks** by Paulet et al., 2023.\n",
        "\n",
        "We suggest reading the methods provided in the article if anything is unclear in this notebook.\n",
        "\n",
        "You can find the preprint here: https://doi.org/10.48550/arXiv.2310.06489\n"
      ],
      "metadata": {
        "id": "zS0y_cGM-S-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to reproduce our results you will also need the annotated datasets we used. You can find them here: (this will soon be provided)."
      ],
      "metadata": {
        "id": "3K5XmyryKsO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that this is still a work in project, we will do our best to provide a simple user friendly code so that anyone can reproduce our results and expend on it."
      ],
      "metadata": {
        "id": "A9t63DpbMROD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also contact me by email using either molina.axel@ens.psl.eu or molina.axel.pro@gmail.com if you have any question, correction or comment."
      ],
      "metadata": {
        "id": "XUWpcuKALa5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation of the dataset"
      ],
      "metadata": {
        "id": "WdNZoeUW9OqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this first part we create a dataset of frames from macaques videos from the field, you can use your own videos if you want !"
      ],
      "metadata": {
        "id": "R816NJeaCPqb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ul0tj7g1Kkr"
      },
      "outputs": [],
      "source": [
        "# connect to your Google Drive, this is where all the following datasets should be\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upgrade FFmpeg to v5.0\n",
        "import os, uuid, re, IPython\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import output, drive\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import os, sys, urllib.request\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
        "\n",
        "from ttmg import (\n",
        "    loadingAn,\n",
        "    textAn,\n",
        ")\n",
        "\n",
        "import os, sys, re\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Cloning Repositories...\", ty='twg')\n",
        "!git clone https://github.com/XniceCraft/ffmpeg-colab.git\n",
        "!chmod 755 ./ffmpeg-colab/install\n",
        "textAn(\"Installing FFmpeg...\", ty='twg')\n",
        "!./ffmpeg-colab/install\n",
        "clear_output()\n",
        "print('Installation finished!')\n",
        "!rm -fr /content/ffmpeg-colab\n",
        "!ffmpeg -version"
      ],
      "metadata": {
        "id": "8lXdqyN--k-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this will extract the frames from all videos in the folder,\n",
        "# here we use a AISARUFULL folder with a sub folder named VIDEOS, edit the paths if needed\n",
        "\n",
        "!cd /content/drive/MyDrive/AISARUFULL/VIDEOS\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/AISARUFULL/VIDEOS\"\n",
        "video_files = [file for file in os.listdir(folder_path) if file.endswith((\".mp4\", \".avi\", \".mkv\"))]\n",
        "\n",
        "for video_file in video_files:\n",
        "\n",
        "  VIDEO_FILE = os.path.join(folder_path, video_file)\n",
        "  os.environ['inputFile'] = VIDEO_FILE\n",
        "  file_name = os.path.splitext(VIDEO_FILE)[0]\n",
        "  os.environ['inputName'] = file_name\n",
        "\n",
        "  !ffmpeg -hide_banner -i \"$inputFile\" -r 1/1 \"$inputName\"_frame%04d.png # we are taking 1 frame / second, you can change this, for exemple for 3 frames second: !ffmpeg -hide_banner -i \"$inputFile\" -r 3/1 \"$inputName\"_frame%04d.png\n",
        "  !mv \"$inputFile\" \"/content/drive/MyDrive/AISARUFULL/VIDEOS/Done\" # this puts all the video that have been extracted in another folder"
      ],
      "metadata": {
        "id": "ZEcLyHmoAPK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection"
      ],
      "metadata": {
        "id": "_gBQy0zm9lw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we train a ML model that will be able to detect macaque faces in images !"
      ],
      "metadata": {
        "id": "nlmrKxdZDBkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following section we will use the previous dataset that was:\n",
        "- annotated using VGG Image Annotator (note that Roboflow can also be used)\n",
        "- data-augmented and split in train and val folders using Roboflow (see the methods in our pre-print)"
      ],
      "metadata": {
        "id": "QRBmNkbk-1Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# please not that most of the following code from \"Detection\" is copied from the \"Detectron2 Beginner's Tutorial\",\n",
        "#some \"#\" are not my own, refer to the original notebook for more details\n",
        "\n",
        "# this is everything we will need to run the detection part\n",
        "\n",
        "# some basic setup:\n",
        "# setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "import torch\n",
        "import torch.cuda\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "wasJyvAt_LKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just set UTF-8 as default, can be changed\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "PHydc2Dqr8BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since our dataset is in a custom format, we need to create a function to parse it\n",
        "# and prepare it into detectron2's standard format.\n",
        "# edit the img_dir to fit the name of your dataset\n",
        "\n",
        "def get_saru_dicts(img_dir):\n",
        "    img_dir = '../drive/MyDrive/AISARUFULL/train/'\n",
        "    json_file = os.path.join(img_dir, \"/json/trainval.json\")\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)\n",
        "        # explanation:\n",
        "        # list(imgs_anns.values())[3] contains the information of the IMAGES (including id)\n",
        "        # list(imgs_anns.values())[4] contains the information of the bounding boxes\n",
        "        # we link the two using the id\n",
        "    dataset_dicts = []\n",
        "    images = list(imgs_anns.values())[3]\n",
        "    annotations = list(imgs_anns.values())[4]\n",
        "    annot_ids = [val['image_id'] for val in annotations]\n",
        "    for idx, v in enumerate(images):\n",
        "        #print(idx) # if you want to make sure it is working !\n",
        "        img_id = v['id'] # for this image, we extract the id\n",
        "        record = {}\n",
        "        filename = os.path.join(img_dir, v[\"file_name\"])\n",
        "        record[\"image_id\"] = v['id']\n",
        "        record[\"file_name\"] = filename # name of each image\n",
        "        record[\"height\"] = 540\n",
        "        record[\"width\"] = 960\n",
        "        #height, width = cv2.imread(filename).shape[:2] # if your images are not standardized\n",
        "        annos_idx = np.where(np.array(annot_ids) == img_id )[0] # to obtain the annotations of the coresponding image\n",
        "        objs = []\n",
        "        for anno in annos_idx: # for each annotations of this image\n",
        "            anno = annotations[anno]\n",
        "            x = anno['bbox'][0]\n",
        "            x = int(x)\n",
        "            y = anno['bbox'][1]\n",
        "            y = int(y)\n",
        "            w = anno['bbox'][2]\n",
        "            w = int(w)\n",
        "            h = anno['bbox'][3]\n",
        "            h = int(h)\n",
        "            obj = {\n",
        "            \"bbox\": [x,y,w,h],\n",
        "            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
        "            \"category_id\": 0,\n",
        "        }\n",
        "            objs.append(obj) # objs is, for an image, a list of detected macaques (bbox, bbox_mode, category_id)\n",
        "        record[\"annotations\"] = objs # record contains filename_, image_id, height, width, annotations. annotations = list of detections (bbox, bbox_mode, category_id)\n",
        "        dataset_dicts.append(record)"
      ],
      "metadata": {
        "id": "KsyMvuDwsMxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOpypkZPPO0b"
      },
      "outputs": [],
      "source": [
        "# here we use the function get_saru_dicts to then be able to train our AI\n",
        "# edit the paths of the different folders if needed, here we use the \"AISARUFULL\" path\n",
        "\n",
        "for d in [\"train\", \"val\"]:\n",
        "    DatasetCatalog.register(\"saru_\" + d, lambda d=d: get_saru_dicts(\"AISARUFULL/\" + d))\n",
        "    MetadataCatalog.get(\"saru_\" + d).set(thing_classes=[\"macaque\"])\n",
        "saru_metadata = MetadataCatalog.get(\"AISARUFULL_train\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# here we define all the hyper-parameters of the model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")) # you can use other models\n",
        "cfg.DATASETS.TRAIN = (\"saru_train\",) # as defined in MetadataCatalog.get\n",
        "cfg.DATASETS.TEST = (\"saru_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 1\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8  # This is the real \"batch size\" commonly known to deep learning people\n",
        "cfg.SOLVER.BASE_LR = 0.001  # pick a good LR (note: we agree that this is a bit vague, we hope it is a good one!)\n",
        "cfg.SOLVER.MAX_ITER = 6000    # 300 iterations is good enough for a toy dataset; you will need to train longer for practical datasets\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # The \"RoIHead batch size\". 128 is faster, and good enough for a toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1   # only has one class (macaque).\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "# and now we train the model\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# and copy the output on our drive\n",
        "!cp --recursive /content/output /drive/MyDrive/AISARUFULL/"
      ],
      "metadata": {
        "id": "snxSRR2z_SBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBXeH8UXFcqU"
      },
      "outputs": [],
      "source": [
        "# look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now run inferences with our model\n",
        "# inference should use the config with parameters that are used in training\n",
        "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "print(cfg.dump())  # print formatted configs\n",
        "with open(\"final6000Weights.yaml\", \"w\") as f:\n",
        "  f.write(cfg.dump())   # save config to file\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "id": "7LpjZ6gbzJgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this part should copy the models weights to your drive\n",
        "\n",
        "cfg = get_cfg()    # obtain detectron2's default config\n",
        "cfg.merge_from_list([\"/content/output/model_final.pth\"])   # can also load values from a list of str\n",
        "print(cfg.dump())  # print formatted configs\n",
        "with open(\"final6000Weights.yaml\", \"w\") as f:\n",
        "  f.write(cfg.dump())   # save config to file\n",
        "\n",
        "!cp /content/output/model_final.pth /drive/MyDrive/AISARUFULL/essaioutput\n",
        "!cp /content/final6000Weights.yaml /drive/MyDrive/AISARUFULL/essaioutput"
      ],
      "metadata": {
        "id": "t5Seen-uze5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can evaluate the performance of the model using AP metric implemented in COCO API.\n",
        "\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"saru_val\", output_dir=\"./output\")\n",
        "val_loader = build_detection_test_loader(cfg, \"saru_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
        "# another equivalent way to evaluate the model is to use `trainer.test`"
      ],
      "metadata": {
        "id": "5pfjkQfg5Hlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "rKKxIRB_9uJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we train a ML model that will be able to identify different macaques of the population in images !"
      ],
      "metadata": {
        "id": "p1gQMLxdDWBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is everything we will need to run the classification part\n",
        "\n",
        "# roboflow to get the datasets\n",
        "%pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# ultranalytics to get the model and train it\n",
        "%pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Image from the Pillow library for image processing\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "6mnSMjylVZYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the next step we:\n",
        "- created a new version of our Roboflow project where 50% of the images were in a train folder, and 50 in a validation folder. (This was trick to randomly split the dataset in half)\n",
        "- took the new train folder and uploaded it on a new Roboflow identification project, adding new annoteded macaques images from other sources to diversify it (This was an easy way to create a standardized dataset with id annotations from two datasets with detection annotations)\n",
        "- downloaded the resulting dataset, from wich all images are then croped in the next *section*"
      ],
      "metadata": {
        "id": "R9vTJXKT_Z6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we constructed a dataset of croped images using Open CV 2, do achieve this we modified a previously used code\n",
        "# please edit the paths like img_dir to select the right folder in your Drive\n",
        "\n",
        "def get_crop_dicts(img_dir):\n",
        "    img_dir = '../drive/MyDrive/AISARUFULL/train/'\n",
        "    json_file = os.path.join(img_dir, \"/json/trainval.json\")\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)\n",
        "        # explanation:\n",
        "        # list(imgs_anns.values())[3] contains the information of the IMAGES (including id)\n",
        "        # list(imgs_anns.values())[4] contains the information of the bounding boxes\n",
        "        # we link the two using the id\n",
        "    dataset_dicts = []\n",
        "    images = list(imgs_anns.values())[3]\n",
        "    annotations = list(imgs_anns.values())[4]\n",
        "    annot_ids = [val['image_id'] for val in annotations]\n",
        "    for idx, v in enumerate(images):\n",
        "        #print(idx) # if you want to make sure it is working !\n",
        "        img_id = v['id'] # for this image, we extract the id\n",
        "        record = {}\n",
        "        filename = os.path.join(img_dir, v[\"file_name\"])\n",
        "        record[\"image_id\"] = v['id']\n",
        "        record[\"file_name\"] = filename # name of each image\n",
        "        record[\"height\"] = 540\n",
        "        record[\"width\"] = 960\n",
        "        #height, width = cv2.imread(filename).shape[:2] # if your images are not standardized\n",
        "        annos_idx = np.where(np.array(annot_ids) == img_id )[0] # to obtain the annotations of the coresponding image\n",
        "        objs = []\n",
        "        for anno in annos_idx: # for each annotations of this image\n",
        "            anno = annotations[anno]\n",
        "            x = anno['bbox'][0]\n",
        "            x = int(x)\n",
        "            y = anno['bbox'][1]\n",
        "            y = int(y)\n",
        "            w = anno['bbox'][2]\n",
        "            w = int(w)\n",
        "            h = anno['bbox'][3]\n",
        "            h = int(h)\n",
        "            # CV2 modification start\n",
        "            idn = anno['id']\n",
        "            imn = anno['image_id']\n",
        "            namen = v[\"file_name\"]\n",
        "            newname = \"/drive/MyDrive/AISARUFULL/box/\" + str(imn) + \"_\" + str(idn) + \".jpg\"\n",
        "            newname = \"/drive/MyDrive/AISARUFULL/box/\" + namen\n",
        "            im = cv2.imread(filename)\n",
        "            roi = im[y:y+h, x:x+w]\n",
        "            down_points = (224, 224)\n",
        "            resize_down = cv2.resize(roi, down_points, interpolation= cv2.INTER_LINEAR)\n",
        "            cv2.imwrite(newname, resize_down)\n",
        "            #print(newname) # if you want to make sure it is working !\n",
        "            # CV2 modification end\n",
        "            obj = {\n",
        "            \"bbox\": [x,y,w,h],\n",
        "            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
        "            \"category_id\": 0,\n",
        "        }\n",
        "            objs.append(obj) # objs is, for an image, a list of detected macaques (bbox, bbox_mode, category_id)\n",
        "        record[\"annotations\"] = objs # record contains filename_, image_id, height, width, annotations. annotations = list of detections (bbox, bbox_mode, category_id)\n",
        "        dataset_dicts.append(record)\n",
        "\n",
        "# then we can crop\n",
        "get_crop_dicts(img_dir)"
      ],
      "metadata": {
        "id": "mbc5qcDi_9SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the next step we:\n",
        "- uploaded the newly croped annotated frames to a new Roboflow identification project, this is \"macaquesid\" in the next section"
      ],
      "metadata": {
        "id": "rKsT2E8t_wZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we take the files from our Roboflow project to create a local dataset,\n",
        "# please edit the variable with the names of your Roboflow project if you use a different dataset\n",
        "\n",
        "rf = Roboflow(api_key=\"2cTPBi7uvIenK2K9kgB0\")\n",
        "project = rf.workspace(\"aisaruid\").project(\"macaquesid\")\n",
        "dataset = project.version(1).download(\"folder\")"
      ],
      "metadata": {
        "id": "mDWbzSVXWJo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our dataset had a few problem, due to errors in the dataset annotating process we\n",
        "# had a few files with 2 macaques id on 1 face. We deleted such files with this code:\n",
        "\n",
        "!rm -rf /content/macaquesID-1/train/Kuro-Tsuwa\n",
        "!rm -rf /content/macaquesID-1/train/Kuro-Tsutsuji-Tsuwa\n",
        "!rm -rf /content/macaquesID-1/valid/Baby_Hiba-Hiba\n",
        "!rm -rf /content/macaquesID-1/valid/Chimaki-Hado\n",
        "!rm -rf /content/macaquesID-1/valid/Tsutsuji-Tsuwa\n",
        "!rm -R /content/macaquesID-1/train/.ipynb_checkpoints\n",
        "!rm -R /content/macaquesID-1/val/.ipynb_checkpoints\n",
        "!rm -R /content/macaquesID-1/test/.ipynb_checkpoints"
      ],
      "metadata": {
        "id": "jhVfmBU-YPGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we select and train the model for 100 epochs, you can change the model you wish to train\n",
        "\n",
        "model = YOLO(\"yolov8n-cls.pt\")\n",
        "model.train(data=\"/content/macaquesID-1\", epochs=100)\n",
        "\n",
        "# check the classify folder that this process created, you will find usefull things like the confusion matrix"
      ],
      "metadata": {
        "id": "cihNIPjI_1zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this gives us the some basic metrics to see how the model performed\n",
        "\n",
        "metrics = model.val()\n",
        "\n",
        "# this is for testing the model on an image\n",
        "\n",
        "model = YOLO('yolov8n-cls.pt')  # load an official model\n",
        "model = YOLO('/content/runs/best.pt')  # load a custom model. note: make sure you select the path to the best weights the model you trained right before\n",
        "\n",
        "# predict with the model\n",
        "results = model('/content/runs/yourfile.png')  # predict on an image, please change the path to the path of the image you want to test the model on\n",
        "\n",
        "for r in results:\n",
        "    im_array = r.plot()  # plot a BGR numpy array of predictions\n",
        "    im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
        "    im.show()  # show image\n",
        "    im.save('results.jpg')  # save image"
      ],
      "metadata": {
        "id": "WlgBMRCI_5IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we used this code to download our results\n",
        "!zip -r IDSaru.zip /content/runs/classify/train8/"
      ],
      "metadata": {
        "id": "SFMn-kYOZJFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Credits"
      ],
      "metadata": {
        "id": "dUDNwtSX_-2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a lot of pre-exiting code for this project, here is a list of what we used (feel free to contact us I we have missed something !)\n",
        "\n",
        "The FFmpeg code for extracting frames was based on this code https://github.com/yunooooo/FFmpeg-for-Google-Drive/blob/master/FFmpeg.ipynb by Yuno on GitHub\n",
        "\n",
        "The detection section was based on the Detectron2 Beginner's Tutorial : https://gist.github.com/indigoviolet/d49b84e153bb58bee809b55dc8d47ee5 by Venky Iyer on GitHub\n",
        "\n",
        "The classification section was based on the codes provided by ultranalytics tutorial on using YOLOv8 for classification tasks : https://docs.ultralytics.com/fr/tasks/classify/\n",
        "\n",
        "Paulet, J., Molina, A., Beltzung, B., Suzumura, T., Yamamoto, S., & Sueur, C. (2023). Deep Learning for Automatic Detection and Facial Recognition in Japanese Macaques: Illuminating Social Networks. ArXiv. https://doi.org/10.48550/arXiv.2310.06489\n",
        "\n",
        "This Notebook was a group effort by Paulet, Beltzung and Molina. Please contact Axel Molina if needed.\n",
        "\n",
        "You can find more informations on the GitHub of the project:\n",
        "https://github.com/AxelCodaeMolina/AI-Saru"
      ],
      "metadata": {
        "id": "6_UsYyZsAF9I"
      }
    }
  ]
}